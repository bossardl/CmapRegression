# Analyse des donn√©es pour former des hypoth√®ses et orienter le choix de mod√®le

### 1. Expertise sur les donn√©es 

1. **Distribution de la cible sup√©rieure √† 0.551446 et inf√©rieure √† 18.406403.**  
   -> Clipping artificiel apr√®s la synth√®se, introduction de forte non-lin√©arit√©.
**Color code:**
- üî¥: Distribution de la cible sur l'enesemble du jeu de donn√©e
- üü°: Evolution de la distribution cible au cours du temps [t;t+10000]


<div align="center">
  <img src="https://github.com/bossardl/CmapRegression/blob/master/histogram_evolution.gif" alt="Evolution temporelle de la distribution" style="width: 50%; max-width: 300px;" />
</div>


2. **Corr√©lations :**  
   - **Forte Corr√©lation :** X8 <-> X1 et X6 <-> X0  
   - **Corr√©lation Mod√©r√©e :** 0<->8<->6, 1<->2<->4<->6, 2<->5, 3<->5<->7, 5<->8, 7<->9  
   - **Faible corr√©lation entre la cible et les variables**
   - **Pas de motif p√©riodique donc une transformation en ondelette ne semble pas une approche prometteuse.**

3. **Fr√©quences et autocorr√©lations n'ont pas abouti.**  
   -> Sugg√®re que la saisonnalit√©/p√©riodicit√© n'est pas √©vidente.

4. **Composantes principales :**  
   - 3 CP > 15% apr√®s PCA  
   - Pas de clusters apr√®s projections sur 2 CP  
   - PC1: X0, X8, X1, X6 ont les plus gros coefficients (>0.3)
     
   -> **PCA ne permet pas d'expliquer la variance simplement, travailler avec les moments statistiques n'est pas prometteur.**

5. **Clustering**
   - Pas de cluster identifiable avec K-Means 3,5,7.

     
### But: Investiguer la distribution de donn√©es √† pr√©dire

* Pas de tendances d'√©volution temporelle  
  --> Hypoth√®se: 

## 2. D√©termination des hypoth√®ses de travail
- Hypoth√®se:  
  - **√âchantillons {$\mathbf{X}_i$} sont ind√©pendants √† courte et longue distance**  
  - **Forte non lin√©arit√©**
  - **R√©duction de dimension non efficace**
  - **la cible n'est pas une s√©rie temporelle**

- Ing√©nierie des variables:
  - Utilisation des donn√©es telles quelles

    
## 3. Choix de mod√®les

* baseline XGBoost: Performances avec des non-lin√©arit√©
* MLP: Mod√®le plus profond pour capturer des relations plus complexes
    - 9729  ok
    - 625  ok
    - 119  X
 
## 4. R√©sultats
* baseline XGBoost: Limitations
-> Ne parvient pas √† pr√©dire la tail de la distribution

* MLP: Parvient √† capturer l'ensemble de la distribution avec faible mse par rapport au XGBoost
    - 9729  ok
    - 625  ok
    - 119  X
 
    - 
## 5. Discussion
D√©couverte et questionnement:
D'o√π proviennent les donn√©es g√©n√©r√©es?
Quelle m√©thode de feature engineering aurait pu aider √† trouver un mod√®le plus petit et performant?

D√©couverte:
Les plus petits mod√®les ont du mal √† g√©n√©rer la tail de la distribution. 
