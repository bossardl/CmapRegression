# Analyse des donn√©es pour former des hypoth√®ses et orienter le choix de mod√®le

### 1. Expertise sur les donn√©es 
0. **Distribution des variables ressemble √† une loi normale avec 100000 points de donn√©es et dim(X)=10**

1. **Distribution de la cible sup√©rieure √† 0.551446 et inf√©rieure √† 18.406403.**  
   -> Clipping artificiel apr√®s la synth√®se, introduction de forte non-lin√©arit√©.
**Color code:**
- üî¥: Distribution de la cible sur l'ensemble du jeu de donn√©e
- üü°: Evolution de la distribution cible au cours du temps [t;t+10000]


<div align="center">
  <img src="https://github.com/bossardl/CmapRegression/blob/master/histogram_evolution.gif" alt="Evolution temporelle de la distribution" style="width: 50%; max-width: 300px;" />
</div>


### üîó 2. Detailed Correlations:
- **Forte Corr√©lation:**
  - **X8** ‚ü∑ **X1**
  - **X6** ‚ü∑ **X0**

- **Corr√©lation Mod√©r√©e:**
  - **X0** ‚ü∑ **X8** ‚ü∑ **X6**
  - **X1** ‚ü∑ **X2** ‚ü∑ **X4** ‚ü∑ **X6**
  - **X2** ‚ü∑ **X5**
  - **X3** ‚ü∑ **X5** ‚ü∑ **X7**
  - **X5** ‚ü∑ **X8**
  - **X7** ‚ü∑ **X9**
- **Faible corr√©lation entre la cible et les variables**

3. **Fr√©quences et autocorr√©lations n'ont pas abouti.**
   -> Sugg√®re que la saisonnalit√©/p√©riodicit√© n'est pas √©vidente. \
   Pas de motif p√©riodique donc une transformation en ondelette ne semble **pas une approche prometteuse**.
   

5. **Composantes principales :**  
   - 3 CP > 15% apr√®s PCA  
   - Pas de clusters apr√®s projections sur 2 CP  
   - PC1: X0, X8, X1, X6 ont les plus gros coefficients (>0.3)
     
   -> **PCA ne permet pas d'expliquer la variance simplement, travailler avec les moments statistiques n'est pas prometteur.**

6. **Clustering**
   - Pas de cluster identifiable avec K-Means 3,5,7.

     
### But: Investiguer la distribution de donn√©es √† pr√©dire

* Pas de tendances d'√©volution temporelle  
  --> Hypoth√®se: 

## 2. D√©termination des hypoth√®ses de travail
- Hypoth√®se:  
  - **√âchantillons {$\mathbf{X}_i$} sont ind√©pendants √† courte et longue distance**  
  - **Forte non lin√©arit√©**
  - **R√©duction de dimension non efficace**
  - **la cible n'est pas une s√©rie temporelle**

- Ing√©nierie des variables:
  - Utilisation des donn√©es telles quelles

    
## 3. Choix de mod√®les

* baseline XGBoost: Performances avec des non-lin√©arit√©
* MLP: Mod√®le plus profond pour capturer des relations plus complexes

 
## 4. R√©sultats
**Approche en rasoir d'Occam, du mod√®le le plus simple au plus complexe**

| Model Type          | Train MSE | Val MSE  | Test MSE |
|:-------------------:|:-------------:|:-----------:|:------------:|
| Linear Regression   | 18.20     | 18.22    | 17.92    |
| Ridge Regression    | 18.20     | 18.22    | 17.92    |
| Lasso Regression    | 18.21     | 18.24    | 17.93    |
| XGBOOST             | 0.0563    | 0.0212   | 0.1598   |
| MLP (10k params)    | 7.942e-06 | **2.467e-05\***| 3.064e-05|
| MLP (1k params)     | 6.843e-06 | 3.007e-05| 2.093e-05|
| MLP (100 params)    | 8.413e-05 | 2.882e-04| 2.811e-04|



*  Linear. Ridge Lasso Regression et XGBoost: Limitations
-> Ne parvient pas √† pr√©dire la tail de la distribution

* baseline Linear. Ridge Lasso Regression et XGBoost: Limitations

* MLP: Parvient √† capturer l'ensemble de la distribution avec faible mse par rapport au XGBoost
    - 9729  ok
    - 625  ok
    - 119  X
 
    
## 5. Discussion
D√©couverte et questionnement:
D'o√π proviennent les donn√©es g√©n√©r√©es?
Quelle m√©thode de feature engineering aurait pu aider √† trouver un mod√®le plus petit et performant?

D√©couverte:
Les plus petits mod√®les ont du mal √† g√©n√©rer la tail de la distribution. 
